{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project to identify Questions in input text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U nltk  # instal the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the network operated by the Duct PTT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did Zhenjin die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The force, therefore, is related directly to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1785 James Hutton presented what paper to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the ctenophora use to swim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text\n",
       "0  What was the network operated by the Duct PTT ...\n",
       "1                               When did Zhenjin die\n",
       "2  The force, therefore, is related directly to t...\n",
       "3  In 1785 James Hutton presented what paper to t...\n",
       "4               What does the ctenophora use to swim"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the input file\n",
    "import pandas as pd\n",
    "import csv\n",
    "data = pd.read_csv(\"test-inputs.txt\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "data.columns = ['body_text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling the data hardcore method\n",
    "import numpy as np\n",
    "label = []\n",
    "Question_words = ['who', 'what', 'when', 'where', 'why', 'whose', 'whom', 'is', 'can', 'does', 'do', 'how']\n",
    "yesnowords = [\"can\", \"could\", \"would\", \"is\", \"does\", \"has\", \"was\", \"were\", \"had\", \"have\", \"did\", \"are\", \"will\"]\n",
    "Interrogative_words = Question_words + yesnowords\n",
    "for i in range(len(data)):\n",
    "    words = data['body_text'][i].lower().split(' ')\n",
    "    if words[0] in Interrogative_words or words[-1]=='?' or words[-1] in Interrogative_words:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the network operated by the Duct PTT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did Zhenjin die</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The force, therefore, is related directly to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1785 James Hutton presented what paper to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the ctenophora use to swim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is the county seat of Duval County, with wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Where is the Asian gold miners strongest in Vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How did france differ from Britain in managing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text  label\n",
       "0  What was the network operated by the Duct PTT ...      1\n",
       "1                               When did Zhenjin die      1\n",
       "2  The force, therefore, is related directly to t...      0\n",
       "3  In 1785 James Hutton presented what paper to t...      0\n",
       "4               What does the ctenophora use to swim      1\n",
       "5  It is the county seat of Duval County, with wh...      0\n",
       "6  Where is the Asian gold miners strongest in Vi...      1\n",
       "7  How did france differ from Britain in managing...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = label\n",
    "data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for imbalance in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    14596\n",
      "1     9699\n",
      "Name: label, dtype: int64\n",
      "0    60.078205\n",
      "1    39.921795\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class_count = data['label'].value_counts()\n",
    "print(class_count)\n",
    "print(class_count/sum(class_count)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not imbalanced as we have enough number of minority samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list of punctuations without '?' so as to remove punctuation from our raw data\n",
    "import string\n",
    "import re\n",
    "new_str_punctuation = ''\n",
    "for char in string.punctuation:\n",
    "    if '?' in char:\n",
    "        new_str_punctuation+=''\n",
    "    else:\n",
    "        new_str_punctuation+=char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(new_str_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Function for cleaning the data which involves removal of punctuations, tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in new_str_punctuation])\n",
    "    tokens = re.findall('\\S+', text)\n",
    "    text = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the network operated by the Duct PTT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>what wa the network operated by the duct ptt t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did Zhenjin die</td>\n",
       "      <td>1</td>\n",
       "      <td>when did zhenjin die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The force, therefore, is related directly to t...</td>\n",
       "      <td>0</td>\n",
       "      <td>the force therefore is related directly to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1785 James Hutton presented what paper to t...</td>\n",
       "      <td>0</td>\n",
       "      <td>in 1785 james hutton presented what paper to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the ctenophora use to swim</td>\n",
       "      <td>1</td>\n",
       "      <td>what doe the ctenophora use to swim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text  label  \\\n",
       "0  What was the network operated by the Duct PTT ...      1   \n",
       "1                               When did Zhenjin die      1   \n",
       "2  The force, therefore, is related directly to t...      0   \n",
       "3  In 1785 James Hutton presented what paper to t...      0   \n",
       "4               What does the ctenophora use to swim      1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  what wa the network operated by the duct ptt t...  \n",
       "1                               when did zhenjin die  \n",
       "2  the force therefore is related directly to the...  \n",
       "3  in 1785 james hutton presented what paper to t...  \n",
       "4                what doe the ctenophora use to swim  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text'] = data['body_text'].apply(lambda x: ' '.join(clean_text(x)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the features from input text with CountVectorizer and TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer Convert a collection of text documents to a matrix of token counts. Link: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Tfidf; Term frequencyâ€“Inverse document frequency: \n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import pickle\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text'])\n",
    "X_count_feat = pd.DataFrame(X_count.toarray()) # Creating a sparse matrix\n",
    "\n",
    "#Save vectorizer.vocabulary\n",
    "pickle.dump(count_vect.vocabulary_,open(\"feature_count.pkl\",\"wb\"))\n",
    "\n",
    "# #n-gram (bigram)\n",
    "# ngram_vect = CountVectorizer(ngram_range=(2, 2)) # search only for bigram\n",
    "# ngram_counts = ngram_vect.fit_transform(data['cleaned_text'])\n",
    "# ngram_counts_feat = pd.DataFrame(ngram_counts.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "X_tfidf_feat = pd.DataFrame(X_tfidf.toarray())\n",
    "\n",
    "#Save vectorizer.vocabulary\n",
    "pickle.dump(tfidf_vect.vocabulary_,open(\"feature_tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(X_count_feat, data['label'], test_size=0.2)\n",
    "# X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test = train_test_split(ngram_counts_feat, data['label'], test_size=0.2)\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf_feat, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_pred, y_test):\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average='binary')\n",
    "    print('Fscore: {} / Precision: {} / Recall: {} / Accuracy: {}'.format(round(fscore,3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the random forest classifier and Evaluation with different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I have only used 15 estimators due to memory issues. But, we can do a grid search to tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with TFIDF features:\n",
      "Fscore: 0.905 / Precision: 0.865 / Recall: 0.949 / Accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=15, max_depth=None, n_jobs=-1)\n",
    "rf_model_tfidf = rf.fit(X_tfidf_train, y_tfidf_train)\n",
    "y_tfidf_pred = rf_model_tfidf.predict(X_tfidf_test)\n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Score with TFIDF features:\")\n",
    "scorer(y_tfidf_pred, y_tfidf_test)\n",
    "\n",
    "# Save the trained model\n",
    "filename = 'rf_model_tfidf.sav'\n",
    "pickle.dump(rf_model_tfidf, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with count features:\n",
      "Fscore: 0.917 / Precision: 0.88 / Recall: 0.957 / Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=15, max_depth=None, n_jobs=-1)\n",
    "rf_model_count = rf.fit(X_count_train, y_count_train)\n",
    "y_count_pred = rf_model_count.predict(X_count_test)\n",
    "\n",
    "print(\"Score with count features:\")\n",
    "scorer(y_count_pred, y_count_test)\n",
    "\n",
    "filename = 'rf_model_count.sav'\n",
    "pickle.dump(rf_model_count, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the fscore which is the harmonic mean of precision and recall, I have chosen count features which is slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "filename = 'rf_model_count.sav'\n",
    "rf_model_count = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#loading the test data: Final-test-inputs data is same as test-inputs.txt\n",
    "test_data = pd.read_csv(\"Final-test-inputs.txt\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "test_data.columns = ['body_text']\n",
    "\n",
    "# Using the same count vocab features in new data as used in training\n",
    "\n",
    "loaded_vec = CountVectorizer(analyzer=clean_text,vocabulary=pickle.load(open(\"feature_count.pkl\", \"rb\")))\n",
    "\n",
    "# Predicitng the ouptut data in batch size of 100\n",
    "start = 0\n",
    "data_size = 100\n",
    "loop_len = len(test_data)//data_size\n",
    "remainder = len(test_data)%data_size\n",
    "y_pred_data_1 = []\n",
    "\n",
    "for i in range(loop_len):\n",
    "    if i<loop_len:\n",
    "        count_feat = loaded_vec.fit_transform(test_data['body_text'][start:data_size])\n",
    "        y_pred_data_2 = rf_model_count.predict(count_feat)\n",
    "        y_pred_data_1 = np.append(y_pred_data_1, y_pred_data_2)\n",
    "        start += 100\n",
    "        data_size +=100\n",
    "    if remainder !=0 and i==(loop_len-1):\n",
    "        count_feat = loaded_vec.fit_transform(test_data['body_text'][(data_size-100):len(test_data)])\n",
    "        y_pred_data_3 = rf_model_count.predict(count_feat) \n",
    "        y_pred_data_1 = np.append(y_pred_data_1, y_pred_data_3)\n",
    "\n",
    "# writing on output file\n",
    "file = open('output.txt','w')\n",
    "for i in range(len(y_pred_data_1)):\n",
    "    if y_pred_data_1[i] == 1:\n",
    "        file.write('True \\n')\n",
    "    else:\n",
    "        file.write('false \\n')\n",
    "file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code can be used for finding best hyperparameter\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [7, 11, 20],\n",
    "#     'n_estimators': [50, 100, 150]\n",
    "# }\n",
    "\n",
    "# # Create a based model\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                           cv = 3, verbose = 2)\n",
    "# grid_search.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "# grid_search.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "# n = [10, 15, 20]\n",
    "# for i in range(len(n)):\n",
    "#     print('For n_estimaotrs :{}'.format(n[i]))\n",
    "#     rf = RandomForestClassifier(n_estimators=25, max_depth=None, n_jobs=-1)\n",
    "\n",
    "#     rf_model = rf.fit(X_tfidf_train, y_tfidf_train)\n",
    "\n",
    "#     y_pred = rf_model.predict(X_tfidf_test)\n",
    "#     y_test = y_tfidf_test\n",
    "\n",
    "#     precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average='binary')\n",
    "#     print('Fscore: {} / Precision: {} / Recall: {} / Accuracy: {}'.format(round(fscore,3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes to save and reuse the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pickle\n",
    "# filename = 'rf_model_tfidf.sav'\n",
    "# pickle.dump(rf_model_tfidf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'rf_model_count.sav'\n",
    "# pickle.dump(rf_model_count, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'rf_model_count.sav'\n",
    "# rf_model = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes for gradientboost classifier which I am unable to currenlty run with my cloud memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = GradientBoostingClassifier(n_estimators=10, max_depth=11)\n",
    "\n",
    "# gb_model = gb.fit(X_count_train, y_count_train)\n",
    "# y_pred = gb_model.predict(X_count_test)\n",
    "# y_test = y_count_test\n",
    "\n",
    "# precision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average='binary')\n",
    "# print('Fscore: {} / Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "#     round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still improve and select other models that fits the data best by hyperparamter tuning with grid search with cross-validation, and checking for other classifiers as well.\n",
    "In this project, i have tried to compare the tfidf and count features of the input text data with rf model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
